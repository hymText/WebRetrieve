学生セッション1:自然言語解析(10:00-12:00)(1) 係り受け解析におけるLeft-corner型遷移
能地 宏,宮尾 祐介人間の文処理のモデル(left to rightで逐次構文木を生成する)。従来法は句構造対象だったが、この研究では係り受けを対象とした。人間が文を理解するときの認知的負荷の性質(右枝分かれと左枝分かれは同じぐらい難しく、それよりも埋め込み文の方が難しい)と似た性質を持つ解析法としてleft-corner parserに注目して、left-corner解析を使った係り受け解析法を考案。18の言語に対して係り受け解析を行い(treebankの正解をもとに解析する)、スタックの深さの分布を調べたところ、他の解析方法に比べてスタックサイズが小さく抑えられ、また言語依存性が小さかった(ので、人の解析方法に近いと主張したいようだ)。言いたいことは比較的明確な気がするが、発表がとっ散らかっていてわかりにくかった。
(2) ベクトルのスパース化を用いたk近傍法におけるハブの軽減
重藤 優太郎,新保 仁,松本 裕治ベクトル空間上での検索において、どのオブジェクトに対しても近いオブジェクト(ハブ)があると適合率の低下を引き起こす(どのクエリに対しても上位に現れるので)。セントロイドに近いオブジェクトはハブになりやすく、また元の次元が高い空間上ではハブが出現しやすい(次元圧縮では解決できない)。従来のハブ抑制法は内積を使う方法でしか有効でないので、距離ベースの方法でも使える方法を提案。手法はベクトルのスパース化(ある次元の要素を0にする)で、スパース化した後にオブジェクトとセントロイドとの距離が大きくなるような要素を0にする。提案方法の精度を対訳抽出で評価。何もしない場合に比べてハブの出現が抑制され、精度は従来法である中心化と同程度。面白い。
(3) 盛り上がり時間帯におけるツイートの言語的特性の解析
藤沼 祥成,横野 光,Pascual Martinez-gomez,相澤 彰子イベントに関するツイート(サッカーとか)に関連するツイートの内容から、イベントの盛り上がり具合を推定する。ツイートの頻度ではなくて言語的特性を使うところがポイント。文字n-gram(n=7)モデルで分析したところ、盛り上がりツイートではクロスエントロピー(=log(PP))が低くなり、また漢字が少ないという特徴が得られた。「相棒」のシャワーシーンで盛り上がりがあるというのが(笑)。異なるイベントの間でも比較的同じようなやり方で盛り上がり識別が可能(分野依存性も高いが)。文字の繰り返しの重要度は(予想に反して)低かった。漢字数とクロスエントロピーは同じものを見てるんだろうか?
(4) 統計的機械翻訳を用いた英語文法誤り訂正の結果をリランキングすることで訂正性能の改善はできるか?
水本 智也,松本 裕治統計的機械翻訳ベースの英語文法誤り訂正。フレーズベースの手法で10-bestを出すと、その中にはよりよい訂正が含まれている可能性が高い。そこでリランキングによって精度向上を目指す。学習データはLang-8、開発・評価データはKonan-JIEMコーパス。通常のSMTの誤り分析と、リランキングによる訂正の可能性について。残念ながら実際にリランキングするところまで行っていない。学生セッション2:音声言語処理(13:30-15:00)(5) 統計的音声対話システムにおける音素系列を用いた頑健な応答選択
佐伯 昌幸,李 晃伸一問一答型の音声対話で、キーワードと応答文のみが与えられた場合のシステム構築法。単語列・キーワードを中間表現とする応答文選択を確率的枠組みで定式化。キーワードと応答分のペア集合からガーベージを生成し、キーワードとガーベージから応答文を選択する確率モデルを構築する。モデル化はCRF。キーワード集合からN-gramによって文を生成し、そこからCRFによって入力と応答分の対応を学習する。また頑健性を増すために、音素系列をCRFの特徴として加えた。入力として書き起こし文+音素を使うと書き起こしだけより性能が上がるが、実入力+音素認識結果だと性能が下がる。CRFの使い方について質問をしたが、やや残念な結果。
(6) ユーザ生成型音声対話システムにおけるクリエイターとユーザの相互刺激によるインセンティブ向上の検討
飯塚 遼,李 晃伸MMDAgentの対話コンテンツをユーザに作らせる研究。対話システムにかかわる人を「ユーザ」と「クリエイタ」に分け、それぞれに対して異なるインセンティブを設定することでシステムの生成と利用を促進する。クリエイタ間では「コンテンツのランキング」、クリエイタ・ユーザ間では「ユーザ評価」、ユーザ間では話題共有によって利用者相互を刺激して、システム利用・コンテンツ作成を促進する。発想は普通だが、対話システム(メイちゃん)の画面に「人気の質問」とか「対話履歴」が表示されるのはなかなかいい感じ。コンテンツ作成のために対話(キーワード・応答文ペア)作成補助のWebシステムなども用意。
(7) 車載用音声対話システムにおけるユーザ負荷を考慮した対話戦略の検討
山岡 将綺,原 直,阿部 匡伸カーナビでの安全性を向上させるため、ユーザが反応しやすい状態(低速運転中、一時停止中)の場合にのみ応答する対話システム。タスクは目的地設定で対話戦略はシステム主導・ユーザ主導・混合主導を切り替える。シミュレーションによってターン数やタスク達成率を測定。その結果、認識率に対するターン数の変化はどの対話戦略でも同じ(絶対的なターン数はユーザ主導が少なくシステム主導が多い)。タスク達成率は、すぐ返答する条件(ユーザ誤認識条件)ではどの対話戦略でもほぼ同じで認識率が下がると大きな影響を受けるが、余裕時発話条件ではタスク達成率の低下が少ない。主観評価実験ではラジコン操作中に対話を行う。余裕があるときのみ発話をする方式では、ユーザの感じる負担は低いが、必ずしも満足度が高いとは限らない。一般セッション1:自然言語解析(15:20-17:50)(8) 形態素解析との同時最適化による歴史的資料の自動表記整理
岡 照晃,松本 裕治歴史的資料では、濁点無表記など表記の揺れが激しい。そのような表記ゆれを自動的に正規化するのが「自動表記整理」。ここでは文字ベースと辞書ベースの方法を併用して精度を上げる。そのために、文字ベースの素性を辞書ベースにマージし、またAugmented-loss Trainingによって品詞タグがなくても学習ができるようにした。文字ベースの手法と比べて精度は上がったが再現率はやや下がり、F値では向上。
(9) 述部機能表現に対する意味ラベル付与
上岡 裕大,成田 和弥,水野 淳太,乾 健太郎「～てしまう」「～かもしれない」などの機能表現についての整理。機能表現意味ラベル付与のためのコーパスを整備し、また既存の機能表現辞書「つつじ」を拡張。事実性解析に応用して評価した。主に事実性解析への応用を意識しながら機能表現の意味ラベルを追加。識別器はCRF。どこがどの機能表現なのかを当てるタスクでは、ベースライン(最長一致)よりはよくなったのだが、そんなに性能は違わない。機能表現の意味まであてるタスクではベースラインよりだいぶ向上。事実性解析に応用したところ、正解ラベル入力に匹敵する性能が出た。
(10)条件付きロジスティック分布を用いた重み付き多タスク学習
濱口 拓男,新保 仁,松本 裕二多タスク学習(複数のタスクの識別機を同時に学習する)の学習法の話だが、最初の「複数の学校での成績」の比喩がよくわからず、結局多タスク学習の前提や枠組みがよく理解できなかった。識別時にタスクラベルが得られない場合に、Mixture of Expertsと同じような考え方でタスクラベルに相当する量を推定し、さらにそのタスクラベル推定も元のマルチラベル学習の中に入れて学習する、らしい。
(11)機械翻訳システムの詳細な誤り分析のための誤り順位付け手法
赤部 晃一,Graham Neubig,Sakriani Sakti,戸田 智基,中村 哲機械翻訳システムを改善するために、翻訳誤りを自動的に検出し、その重要度を自動付与する。基本的に、訳出した文中のn-gramに対して様々な重みを考えて、n-gramごとに正しいかどうかを評価して抽出する。重みとして、頻度や自己相互情報量、条件付確率、識別言語モデルの重みなどを検討。結果として、頻度と相互情報量の性能はランダム以下。条件付確率と識別言語モデルに基づく手法がよい性能だった。分析の結果、削除・並べ替え誤りの検出精度は識別言語モデルの方法がよく、それ以外は条件付確率の方法がよかった。
(12)言語資源の追加:辞書かコーパスか   森 信介,ニュービッグ グラムLRECの発表内容をそのまま発表(NL研ではそのような発表を推奨しているらしい)。コーパスベースNLPのドメイン依存性。それを改善するために言語資源を追加することを考えると、辞書項目を追加する(手軽だが限定的)ことと、コーパスを追加する(再学習が必要だがコンテキストの情報が使える)ことを比較。CRFのような手法だと系列全部に完全なアノテーションが必要だが、pointwiseな手法だと再学習したいところだけにアノテーションがあればよい。タスクは形態素解析で、システムとしてCRFベースのMeCabと、pointwiseのKyTeaを比較。実験条件は追加なし、辞書項目のみ、辞書+関連部分の再学習、コーパス追加+再学習の4つ。単純な辞書追加とコーパス追加を比較すると、辞書追加だけでもコーパス追加の80%ぐらいの性能向上が得られる。BCCWJドメインでは再学習の効果は限定的だが、レシピドメインでは再学習によって性能が上がる。また、特許ドメインで(部分的)アノテーション作業の時間と精度を調べたところ、12時間アノテーション作業を行って、一般ドメインの性能に近い性能が得られた。この後学生奨励賞の発表。(2)の重藤さん、(6)の飯塚さんが受賞。おめでとうございます。
9:45-11:00 索引付け距離順STDにおける索引の検索効率改善と複数検索語の同時検出大野哲平, 秋葉友良(豊橋技科大)あらかじめ認識音節列と全音節との距離マトリクスを作っておいて,検出時に距離の近いところから投票により単語候補を見つけ,最後にDTWで検証する方法.基本的な方法では,最初の検出時には音節の挿入脱落は考えていないので,音節挿入脱落によって性能が下がることがある.そこで投票方法を工夫することによって第1段階で候補が漏れてしまうことを防ぐ.DTW全探索とほぼ同じ性能で速度は30倍.また,複数の検索語を同時に検出して距離順に提示するアルゴリズムを提案.前の手法に対して,「投票時に工夫するのではなく,投票結果の時系列の集計の仕方で同じことができるのではないか」という中川先生の指摘.鋭い(けど,よく考えるとちょっと違うような).Suffix Arrayを用いた高速STDのための検索閾値の調整手法三浦成一, 桂田浩一, 入部百合絵, 新田恒雄(豊橋技科大)パワポのバージョンの問題でうまく映らずちょっとごたごたした.1つの単語を複数の部分に分けて,サフィックスアレイによる高速検出を行う方法で,分割されたそれぞれの部分から検出される候補数にばらつきがあるので,候補数をそろえるために各部分単語に対する検出閾値を調整する.今回の方法では閾値を段階的に緩和する方法(反復深化的探索)を使うので,前の検索結果での候補数から次に閾値を緩めたときの候補数を推測し,それが各部分で同じになるようにした.また,音素の置換誤りと挿入脱落誤りの候補に分けて推測を行ったり,音素系列に依存した推測を行ったりした(こちらはあまり効果がない).ビット演算に基づく高速な音声ドキュメント検索語検出北研二(徳島大学), 松本和幸(徳島大学), 吉田稔(徳島大学), 柘植覚(大同大学), 北岡教英(名古屋大学), 武田一哉(名古屋大学)北先生が自ら発表.ドキュメントを認識した音節列に対して,音節のn-gramをハッシュ関数で128bitのビット列にする.同じようにクエリの音節列をハッシュのビット列にして,ハッシュ値同士のハミング距離によって音節系列間の距離を計算する.ハッシュ関数は,音節の全種類それぞれ1ビットずつ割り当てて,その音節が存在する場合はビットを1にするという簡単なもの.これであたりを付けて最後はDPマッチング.ビット演算で距離計算ができるので高速.総額10万円未満のノートPCでも動く.3/15 11:10-12:00 音声中の検索語検出ベクトル量子化に基づいた音声中の検索語検出における検出結果の統合坂本伊織, 工藤祐介, 山下洋一(立命館大)入力音声のMFCCをセグメント量子化し,そのVQコード列を照合する.音素とVQコードの対応は話者依存.また,サブワードのフレーム長分布もモデル化し,検出するときのスコアに加える.さらに,複数の条件(コード対応の学習条件:単語か音節か,N-bestがあるか)で検出を行った結果を統合することで性能が向上.キーワード集合をクエリとする最良照合STD方式堂元健太郎, 宇津呂武仁(筑波大), 古屋裕斗, 西崎博光(山梨大)入力音声を認識して音素遷移ネットワーク(音素単位のconfusion network)を作り,それに対して音素列をマッチングして索引を作る方式.過照合による適合率の低下が問題.提案法では,単一のキーワードだけでなく,キーワード集合をあらかじめ用意しておき,それに対する索引を作るのが特徴.キーワード集合をあらかじめどう作っておくかは不明.キーワード集合の各キーワードに対してマッチングをして,時間的に重なるキーワードのうちスコアが低いものを削除する.CSJに対する実験では,学会講演については改善しなかったが,模擬講演については改善した(未知語率が高いため).また,山梨大収録の模擬講義音声についてはちょっと改善.さらに改善手法として,競合候補の中でスコア最良のものを競合スコア全部に付与する方法を試して,さらにちょっと改善.なんか全体的な検索システムの枠組みがよくわからん.13:00-14:15 音声クエリ分布間距離ベクトル表現による音響的類似度を利用したテキスト及び音声クエリからの音声検索語検出の改善牧野光晃, 山本直樹, 甲斐充彦(静岡大)学生さんにがついてるけど甲斐先生が発表.サブワード単位での距離計算によるスポッティングに加えて,2パス目でHMMの状態間のバタチャリヤ距離(BD)または各状態からすべての状態へのバタチャリヤ距離のベクトル(DDV)の間のノルムで詳細な距離計算を行う.最終的にはBDとDDVを両方使う(BD-DDV)手法が有効.音声クエリの複数認識結果を利用した音声ドキュメント中の検索語高速検出法坂本渚, 中川聖一(豊橋技科大)音素認識結果のn-gramインデックスを使ったSTDで,複数の音響モデル(GMM-HMM1種とDNN-HMM2種)からの認識結果を組み合わせることで検出性能の向上を図る.合わせ方は「単に複数の候補を合わせる」方法と「多数決をとる」方法を比較した.どちらもRecall-Precisionカーブの端の方ではやや差が出るが,あまり変わらない性能.音声ドキュメントからの頻出発話語句の発見米倉千冬, 古屋裕斗, 澤田直輝, 名取賢, 西崎博光, 関口芳廣(山梨大)音声中で何度も発話されている単語を検出する.そういう単語をリストアップすることで,音声ドキュメントに対する(人間が参照するための)索引を作るのが目的.方法としては,連続音声認識結果から音素遷移ネットワークを作り,その中から頻出単語を検出するという3パス.マッチング処理は,ネットワーク同士のIRIFCDPみたいな手法.対応する音素列がわかったら,その音素列がもともと属していた単語認識結果を使う方法と,音素列をかな文字列にしてかな漢字変換を使う方法の2つを試した.実験の結果では,全体的には再現率が上がらない感じ.評価の仕方がおかしい気がする.14:25-15:40 音声内容検索段階的検索と擬似適合性フィードバックを用いた講演音声ドキュメント検索南條浩輝, 西尾友宏, 吉見毅彦(龍谷大学)疑似適合性フィードバック(PRF)によるクエリ拡張を行うと,検索単位(パッセージ)が小さいときには効果があるが,単位が大きい(全講演が1単位とか)場合にはかえって性能が悪化する.そのため,PRFでクエリ拡張をするための単位としては短いパッセージを使い,それでクエリを拡張したうえで長い文書単位の検索を行う.さらに最初のパッセージ検索を高精度化するため,クエリ拡張のための最初の検索において,まず長い単位で検索を行って文書を選んだうえで,その中のパッセージを2段目で選ぶ.また,最初の検索から関連語を抽出する場合に,検索された複数のドキュメントを1つにまとめて関連語を抽出するのではなく,それぞれのドキュメントから抽出された検索語のANDをとってクエリ拡張に使う.最後にクエリ拡張した結果としない結果を統合して性能を上げる.種々のテキスト検索モデルの頑健性向上による音声ドキュメント検索の高精度化北岡教英(名古屋大学), 市川賢(名古屋大学), 柘植覚(大同大学), 武田一哉(名古屋大学), 北研二(徳島大学)検索精度を上げるための地道な努力.ベクトル空間モデル,クエリ尤度モデル,適合モデルの3つをそれぞれ改良.ベクトル空間とクエリ尤度モデルについてはWeb文書を使ったクエリ拡張を行う.適合モデルでは,Web文書から検索質問に関連する単語で検索質問のモデルを拡張.適合性と文書構造を併用した音声ドキュメント検索における適合音声区間決定加瀬健太, 秋葉友良(豊橋技科大)パッセージ検索のための文書分割.従来のように固定長の区間に区切るのではなく,テキストタイリング(Hearst法)および文末表現を使って文書を分割する.分割されたそれぞれのセグメントに対して索引を作って検索を行う.検索時には,適合度が一番高いセグメントをまず見つけて,そこを中心に発話単位またはセグメント単位で様々な長さの区間を設定して適合度を測り,最も適合度の高い区間を最終結果とする.さらに,セグメント分割の時に使う「セグメント境界スコア」も適合度スコアに加えて評価する.検索方法はクエリ尤度モデルと適合度モデル.テキストタイリングよりも,文末表現によるセグメント分割の方が高性能.境界スコアを利用する方法は性能改善せず.
10:20-12:00 音声ドキュメント処理Automatic Speech Recognition and Machine Translation System for MIT English Lectures using MIT and TED CorpusVeri Ferdiansyah, Seiichi Nakagawa(Toyohashi Univ. of Tech.)MITコースウェアに字幕を付けるため,音声の自動書き起こしと翻訳を行う.MIT OpenCourseWareの音声で適応(30人,5時間).適応手法,言語モデルの学習データ(WSJ, MIT OpenCourseWare)を比較.翻訳モデルでは,日本語側でCSJ, TED Talks, JENAADを組み合わせて利用.認識ではWSJに対してMAP推定でMITコーパスを適応させたものがよかった.翻訳の学習データとしては,TEDだけ使った場合が最良.WERが32%ぐらい,BLEUで7.16ぐらい(重要文のみ).チャレンジングなタスクだが,結果はまだまだな感じ.音声認識結果の有用性の自動判定に基づく講義のリアルタイム字幕付与システム桑原暢弘, 秋田 祐哉, 河原 達也(京大)音声認識人手修正の字幕作成を効率化するため,認識結果に人手修正が必要なのかどうかを識別して,その結果を修正者に提示する.認識結果の正誤よりも「字幕として修正が必要かどうか」を基準にするところが特徴.文節単位で「有効」「無効(フィラーなど)」「要チェック」の3クラス分類.認識は単語単位なので,CSJから学習したSVMで単語系列を文節にまとめる.まとめたものが文法的な文節構造になっていないものは要チェック.それ以外のものはCRFで3クラス分類.CRFの素性としては,文節そのものと読み,品詞,信頼度を使った場合が最高性能で約80%.「要チェック」の再現率は6割.「無効」の判定はうまくいかない(データが少ないため).また,字幕作成方法(認識修正提示,認識提示修正を上書き,それに自動分類を入れたもの)を比較.聴覚障碍者によるプリファレンスは従来法と同程度だが,字幕作成者の負荷が下がるので有効という主張.聴覚障碍者へのリアル講義での字幕提示にプロンプターを使っているのが面白い.Normalized web distanceを用いた音声認識誤り訂正法エンフボロル ビャムバヒシグ, 田中克幸, 滝口哲也, 有木康雄(神戸大)音声認識結果からConfusion Networkを作って,各位置の1位の単語についてCRFで正誤判定.誤っていると判定されたら2位を取ってくる.CRFの素性として従来法ではLSAのスコアなどを使っていたが,ここではNormalized Web Score(検索エンジンのヒット数に基づく単語共起の強さ)を使う.また,CNのヌル遷移をそのまま使った場合と使わなかった場合の2つのCRFを併用する.LSAを利用してCRFを1つしか使わないものと比べて3.45ポイント改善.第三話者読み上げコーパスによる音声ドキュメントの活用小泉悠馬, 塩出萌子, 伊藤克亘(法政大)ひさしぶりに伊藤先生を生で見たが,あいかわらずで安心した.アニメで声優が変わった時に,新声優の声を旧声優の声に近づける話者変換.演技はそれぞれ妥当だと思われるので声質だけ変換する.パラレルコーパスがとりにくいので,第3話者を経由して2段階で変換を行う.中間の話者は,できるだけ対象話者・目標話者のまねをして話す.主観評価による結果はあまりよくない.対象がドラえもんなので,「だみ声」などがうまく変換できていないのではないかと分析.13:00-18:00 「中川聖一先生退職記念講演・討論会」「音声処理における人間とコンピュータの能力の違い」中川聖一(豊橋技術科学大学)・人間対コンピュータでコンピュータが勝ってきた(そしてまだ人間が勝っている)歴史 チェス,クイズ,将棋 vs. 囲碁,入試,翻訳,対話 それぞれの探索空間の大きさ・音声に含まれる情報量(1発話3秒あたり) 位置:5bit 感情:3bit 話者:7bit 言語:42～bit・音源分離能力 PASCAL CHiME challange 最高性能のシステムは人間より1～4%程度の劣化・音源同定(発声方向,距離) 2つのマイクロホンアレイを使うと人間の方向同定性能を超える 距離の同定性能は機械が人間を超える・感情認識 人間が8種類の識別60% vs. 機械が4種類の識別70%・話者認識 人間<機械,人間の結果統合≒同じ内容の機械識別,人間の結果統合<違い内容の機械識別 位相情報の利用,MFCCと位相の併用・音声認識 機械は人間の能力に全く及ばない(特に雑音・残響環境下).ロバスト性が違う 前後のコンテキスト付与による単音節単語の聴取率:音響環境で20ポイント程度,前後の単語でさらに10～20% 音韻認識率・パープレキシティと単語の認識率の関係 相互情報量と言語重み最後に中川先生の4月からの身の振り方を紹介して終了.伊藤克亘先生から「ここで言っている人間のレベルはどの程度のものなのか?」という話が.「音声・画像処理の共通点と統合・変換処理について」有木康雄(神戸大学)・音声と画像の認識対象 どちらも似ている(Who,How,Where,When,What)が,Why, Contextなどについての研究(意図理解,状況理解)は少ない・生成 音声の生成モデル,ケプストラム分析          室伝達関数 画像の生成モデル(入射光束と反射光束),画像信号のケプストラム          撮像モデル,点広がり関数(PSF)・特徴量 画像にしかないSIFT特徴量(回転・拡大縮小・輝度変化に不変)・検出 AdaBoostによる顔検出,HOG特徴量・追跡 パーティクルフィルタ・認識 HMM,GMM,SVM, Convolutional NN
ImageNetの認識コンテストILSVRC
BoF 特定物体認識(検出),一般物体認識・変換 声質変換 GMMによるHOGの視覚化(HOGから元画像を推定する)・合成 AAMによる顔合成・認識(顔の3次元形状の主成分分析・フィッティング)・音声画像処理の相互流用 GMM話者変換GMM超解像へ AdaBoostによる顔検出要求発話検出 言語文法映像文法・今後 機能的パターン認識 シンボルグラウンディング 状況の理解,意図理解"Performance Comparision of the Front-End Noise Reduction Methods with Parallel Model Combination Approach"Hyun-Yeol Chung(嶺南大学)鄭先生ひさしぶり.最初に韓国の音声認識の現状.・韓国政府のICT戦略:Smart Software, IoT, BigData/Cloud, UI/UX for Mobile・Google, Nuanceなどの台頭により韓国での研究は急速に縮小・Google音声検索,Siriの成功により研究動向に大きな影響 言語モデル,対話管理,NLPなどの重要性,ビッグデータ,自発音声認識・研究機関:メーカー(Samsung, LG, Pantech)ポータル(Naver, Daum, Google) 通信(SK Telecom) 研究機関(ETRI, KIST) 大学(KAIST, SNU, Postech)など.産学連携も・モバイルのための連続音声認識に注力・研究課題:データ収集,言語モデル,対話処理 データ収集:一般に利用可能なコーパスがない(商用のみ,会社は自分で集めている) 言語モデル:データベースがないので推定ができない,Large-scale distributed LM 対話管理・企業の研究の現状 サムスン:自前でエンジンを開発(S Voice) Siriのようなアシスタント サムスンのスマホに入っている      (2011年にVlingoから買ったもの) LG:長い歴史,自前で開発 (Q Voice)   韓国語エンジンWernicke Naver:認識エンジン Link 13言語をサポート Daum:ベンチャー会社Dialoidを買収,音声アプリ開発 ETRI:多言語自動翻訳 GenieTalk・Siri(Apple), S Voice(Samsung), Q Voice(LG), Smart Voice(Pantech) の比較ビデオ(韓国のTV番組) 4つのスマホに一斉に「おなかすいた」と呼びかけて反応を比較,など.Smart Voiceはできない子・自前の技術が必要,NLPと対話管理が必要,データを集めなければならない後半は鄭先生の研究紹介で,PCMによるフロントエンド雑音除去.・さまざまなフロントエンド・バックエンドの雑音対策を比較・試した中ではTwo-stage Mel-warped Wiener Filter (TMWF)がよかった・RASTAとARMA filteringの比較・PMCとの組み合わせ さまざまな雑音に対して,どの手法の組み合わせがよいか評価・フロントエンドでは ERN-CMVN-RASTA PMCの組み合わせではTMWF-PMCがよかった「音声情報処理の近未来像ー音声・音響・言語処理の最前線からみる音声言語処理の今後の展開ー」司会: 中川聖一パネラー: 伊藤彰則(東北大), 篠田浩一(東工大), 西村雅史(IBM東京基礎研究所), 峯松信明(東大)出演したのでメモがとれず.
学生セッション［4S 会場］(3 月12 日(水)14:30 ?17:00)音声対話システム・音声認識 座長 伊藤 彰則(東北大)1 音声対話システムのための周辺の文脈に着目したドメイン固有語のカテゴリ特定 藤巻寛継,駒谷和範,佐藤理史(名大)案内対話での未知語(店の名前)に対するカテゴリ推定.ドメインはレストラン案内.機械学習のデータを作るため,大規模コーパスの中n含まれる店の名前の周辺単語の特徴からCRFでカテゴリ(レストラン名かどうか,等)を推定する.コーパスはYahoo!知恵袋のグルメレシピコーパス.普通のNLPでのカテゴリ推定と違うのは,当該単語の表記を使わないことだけらしい.2 マルチモーダル情報を利用した未知語を含む発話のドメイン選択精度の向上 高橋裕己(早大),中野幹生(早大/ ホンダRIJ),岩橋直人(京大),左 祥(京都工繊大),船越孝太郎(ホンダRIJ),岡 夏樹(京都工繊大),菅野重樹(早大)オフィスロボットなどとの音声対話でのドメイン推定.未知語を含む場合を考えて,未知語を含んでもよい文テンプレートを使って認識をすることと,画像情報からのユーザのジェスチャなど(ポーズ,物を持っているか,等)を使うことが特徴.それぞれの結果を特徴量とし,SVMとロジスティック回帰により識別.ドメインは5つ.文型と画像を組み合わせたことによる効果は見られない.3 簡略表現を認識し応答に利用する音声対話システム 秋田谷 樹,駒谷和範,佐藤理史(名大),中野幹生(ホンダRIJ)簡略表現(ケンタッキーケンタなど)が使える音声対話システム.ドメインは名古屋市内のコンビニ・レストラン検索.施設名からルールによって簡略表現の候補を自動生成し,人手で補って「簡略表現リスト」を作成し,それを使って音声認識用の辞書と言語モデルを作る.またこれを使って,「語彙の引き込み」についての調査を行った.通常の施設名称(コンビニだったらチェーン名など)をシステムが使った場合には,ユーザも同様の単語を使う傾向が高い・・・ということなのだけど,実験結果から本当に引き込みが起きたのかどうかいまひとつわからない結果.4 連続2ターンのユーザ発話を用いた発話内の未知語有無推定 大塚嗣巳(名大),中村友昭,中野幹生(ホンダRIJ),駒谷和範(名大)発話に未知語が含まれているかどうかを識別する.特定の1発話を使うのではなくて,ユーザの連続する2発話を使うことが特徴.1発話目に未知語が入っていれば,聞き返しをすることで2発話目にも同じ単語を言うだろうという仮定に基づく.特徴量は,それぞれの発話での単語信頼度や,2発話での単語信頼度の差などを使う(最大75).ドメインは「物の名前を教わる」など.2発話を使うことにより,未知語存在の判定制度が80%ぐらいになる(従来法は73%ぐらい).面白い.5 (講演取消)6 混合方言言語モデルと混合比推定による方言音声認識システム 平山直樹,吉野幸一郎,糸山克寿,森 信介,奥乃 博(京大)一つの発話の中に複数の方言が混ざってしまっている場合に,単一の方言言語モデルでは難しいので,方言の混合言語モデルを使う.方言は5カテゴリ(北奥羽,東京,近畿,東山陽,肥筑).学習データは小規模な対訳コーパスから学習したルールによる疑似方言コーパス.複数の方言に対して,さまざまな混合比の言語モデルを作っておいて,すべてのモデルで認識をしたうえで,尤度最大の結果を採用する.複数の言語モデルについての認識尤度と正解精度には高い相関がある.また,各単一方言言語モデルでそれぞれ結果を出してROVERで統合する方法も試した.(前言語モデルを使う方法よりやや性能が低い).7 多人数対話システムにおけるロボットの挙動に対するユーザ反応の分類 水野 壮,駒谷和範,佐藤理史(名大)ロボットが入った多人数会話において,ロボットが正しい反応をした時と間違った反応をした時に,周りのユーザがどういう挙動をするかを分析した.ロボットが正しい動作をした場合は「うなづく」動作が多く,誤った動作をした時は「笑い」の動作が多い.また,これを利用するため,GMMによって笑い音声の検出を行った.GMMによる方法はいまいち.8 音声発話の誤分割修復のための連続する発話対の同一発話判定 堀田尚希,駒谷和範,佐藤理史(名大),中野幹生(ホンダRIJ)この話はこの前聞いた気がする.ユーザ発話でポーズが挟まったために発話が分割されてしまったときに,分割された発話を修復すると同時に,発話が終わってからシステムが応答するようにする.この発表では,複数の発話断片に対して「修復が必要なのかどうか」を識別する.識別は決定木.特徴量は信頼度や雑音判別スコア,発話間の時間間隔,発話の音量など.特徴選択として,性能向上とドメイン独立性の両方を考慮する.最終的な識別性能(ドメイン独立)は86%.9 一問一答型音声対話システムにおけるシステムからの自発的な発話生成 吉田達平,駒谷和範,佐藤理史(名大)一問一答型の音声対話システムで,より長く対話が続けられるような方策.ユーザー入力に対して決められた発話を選んで出力するだけでなく,それら(直前の入力と出力)から「自発的な発話」を生成して出力する.「自発的な発話」は応答文データベースの中から選ぶが,選び方として「直前の応答文と似ている応答文を選ぶ」方法,「直前の発話候補文に似ている発話候補文に対応する応答文を選ぶ」「それらの統合(類似度が高い方を使う)」を試した.統合手法を使うことで,適切な自発発話を選ぶ率が高くなる.それにしても,長く対話を続けるのが目的なのに一問一答型の対話システムを使うってどうなの.10 ヒューマンロボットコミュニケーションにおける意味学習機構の有用性 沢登京介,梅崎太造,田口 亮(名工大),保黒政大(中部大)ロボット対話システムでの意味学習が,ユーザの印象にどういう影響を与えるかの調査.ロボットは実ロボットではなくてSIGVerseを利用.単語を教えるには「これの名前はXです」という表現(Xは音節連鎖)を使う.学習した単語とオブジェクトの対応を記憶させ,以降のコマンドに使えるようにする.タスクは積み木の世界でのロボット操作.デモ映像があったが,合成音声が子供の声.被験者に自由に対話させると,学習しないロボットを使う時の方が発話数が多い(名前を付けられるロボットの方では,名前を考えるのに時間がかかるため).学習有のロボットの方が印象がよかった.最後に学生奨励賞の贈呈.迷ったが,4番の大塚さんと6番の平山さんに贈呈した.おめでとうございます.
第5会場 スペシャルセッション 音楽音響 [音楽音響と音楽関連産業]午前−前半(9:15～10:30)［音楽音響と音楽関連産業II］ 座長 三浦 雅展 副座長 安井 希子3-5-1 (招待講演)音楽情報検索の現状および音楽産業への展開について 帆足 啓一郎(KDDI研究所)音楽検索関連研究の「失敗」の紹介。・2000年代前半ぐらいに開始(それまではテキスト検索だったが、Googleの出現で終了)・「にたうた検索」 手持ちのMP3と似た楽曲を検索する Query-by-Example、適合フィードバックによる個人嗜好適応 実験サイトでの公開本格採用に至らず ユーザニーズとの乖離  未知の楽曲を探す、という行動をユーザがするのか?  特定の楽曲を探すなら厳密検索の方が適切 検索プロセスが面倒だった 「結局、検索は一つの機能に過ぎない」「検索は楽しくない」技術だけでサービスが実現できるという勘違い・音楽検索から「音楽の発見・活用」へ まぜうた(にたうた検索+自動リミックス(ビート合わせ))・楽曲スライドショー 楽曲の歌詞印象に適した画像を検索スライドショー 歌詞の行から画像検索をして並べる  そのままやると変になるので、画像全体のキーワードと行ごとのキーワードを併用・今後の展開? Context-basedな音楽検索・再生(ドラえもんの「ムードもりあげ楽団」) ・まとめ 研究テーマのアウトプットは明確に考えるべき ピボットを恐れずにやるべき3-5-2 (招待講演)歌声分析のエンターテイメント応用:音楽検索からカラオケまで 伊藤 彰則(東北大学)私の発表。前半はこれまでやってきたハミング・歌声検索について。後半では最初にカラオケの歴史と、もっとも古いカラオケ採点関係の特許、カラオケ関係の面白特許など。3-5-3 擬音語音声入力による効果音検索システム 渡邉 智之,相川 清明(東京工科大)擬音語による効果音データベースの検索。効果音にあらかじめ擬音語を付けておき、音声の認識結果とDPマッチングで対応を付けて検索する。新しくないような気が・・・実験の結果、試行の2/3で目的の効果音が見つかった。午前−後半(10:45～11:45)［音楽音響と音楽関連産業III］ 座長 西村 明 副座長 江村 伯夫3-5-4 2次元LRパーサによる音楽演奏MIDI信号からの自動採譜 高宗 典玄,亀岡 弘和(東大院情報理工),嵯峨山 茂樹(東大院情報理工 / 現:国立情報学研究所)MIDIを楽譜にする。楽譜を2次元PCFGで表現し、テンポ変動を状態遷移モデルで表現したうえ、組み合わせて生成モデルを作る。2次元PCFGは、音符を時間方向と和音方向に分割していくモデル。テンポについては、複数のテンポを異なる状態で表現し、テンポが似ているほど遷移確率を高くする。パージングは富田パーサを2次元に拡張したもの。そのために、複数の音高に対するスタックを並列に用意して、複数のスタックに対するshift-reduceが同期するようにしなければいけないので結構めんどくさそう。入力に対して複数のテンポを仮定しながらパージングを行い、スコアでビームサーチをしながら候補を絞り込んでいく。3-5-5 音楽音響信号に含まれる調波音の周波数特性とドラムの音色の転写システム 中村 友彦(東大院・情報理工),吉井 和佳,後藤 真孝(産総研),亀岡 弘和(東大院・情報理工)任意の(ポピュラー音楽の)打楽器の音を、別な音楽に含まれる別な打楽器の音に挿げ替える。最初にスペクトログラムから調波音と打楽器音を分離し、打楽器音を楽器ごとに分離したうえで別な打楽器音にすげかえる。挿げ替える方法として、単にコピーする方法と、イコライズによって音色を似せる方法を検討。コピーする方法では、楽器分離で使ったNMFのアクティベーションを対応付ける方法を使う。全体としてはコピーした方が評価が高いが、イコライザの音が好きな人もいる。3-5-6 混合音楽信号の正弦波・残差モデルを用いた再生速度変換の検討 五十嵐 佑樹(東北大院),伊藤 仁(東北工大),能勢 隆,伊藤 彰則(東北大院)うちの研究室の五十嵐君。音楽信号を正弦波成分+雑音成分に分け、雑音成分をLPC分析したうえで、アタックの部分以外を伸び縮みさせることで、楽器の時間的構造を変えないで再生速度を変える。話はわかりやすかったと思うが発表が長すぎ。3-5-7 調波時間因子分解に基づく音楽事前情報付き多重音解析 四方 紘太郎(東大・工),高宗 典玄,中村 友彦(東大院・情報理工),亀岡 弘和(東大院・情報理工/NTT CS研)新たな多重音解析手法「調波時間因子分解」を提案。特殊なウェーブレット変換(HTCと類似な結果を得る)とNMFに類似の規定・アクティベーション分解を使う。F0推定を含み、ビブラートなどのF0変動を扱うことができる。また、調推定結果を使ってスペクトルの分布関数の事前分布を調整することによって精度を上げる。
午前−前半(9:30～10:30)［音響特徴量I］ 座長 柘植 覚 副座長 篠原 雄介座長が来られず,代理.1-4-1 Using Phonetic Context for Continuous Speech Recognition with Invariant Structure張 聡穎(東大・日本IBM),鈴木 雅之,倉田 岳人,西村 雅史(日本IBM),峯松 信明(東大)構造的特徴を使った連続音声認識の認識候補のリスコアリング.従来は音響モデルとしてモノフォンを仮定していたが,音素コンテキスト依存モデルを利用することを検討.構造的特徴を使うにはすべての音素モデル間の関係を記述しなければならないので,triphoneをそのまま使うと破たんするため,音素コンテキストをクラスタリングして数を減らす.LVCSRタスクでは,音素モデルを90ぐらいまで減らし,文字誤り率で0.2ポイント強改善.1-4-2 補助関数法による制約付きボルツマンマシンの学習アルゴリズムの検討高宗 典玄,石原 達馬(東大院情報理工),亀岡 弘和(東大院情報理工 / NTT CS 研)RBMの学習アルゴリズムとして補助関数法による方法を導出.IBIS的内容(実験は生成データなので音声とは関係ない).補助関数法を使うことが従来法と比較してどうよいのかについての原理がよくわからなかった.うまくハイパーパラメータを設定すると,従来法であるCD法よりも収束が速い.ここで講演取り消しのため15分休憩1-4-4 音響特徴量を利用した吹き出しテキストの生成松宮 翔,サクティ サクリアニ,ニュービッグ グラム,戸田 智基,中村 哲(奈良先端大)音声認識結果を使って映像に字幕をつける際に,パラ言語情報を使って吹き出しの形を変えて表示.吹き出し字幕の付与対象はアニメ(ワンピース).吹き出しは通常(丸い)とギザギザの2つを自動分類.特徴抽出にはopenSMILEを利用し,SVMで識別.音響特徴量と言語特徴量を組み合わせることで85%ぐらい吹き出し分類が可能.主観評価もしているが,声が実際に聞こえているのにさらに吹き出し表現を変えることにどの程度意味があるのかなあ.---第4会場 スペシャルセッション 音声A/音声B [音声技術と画像・動画処理の接点−基本から適用事例まで−]1-4-9 (招待講演)コンピュータビジョンの最近の研究動向 岡谷 貴之(東北大)人間の視覚系とコンピュータビジョン.人間の視覚系(背側=空間認知,腹側=物体認知)と合わせたコンピュータでの技術.・多視点画像からの3次元モデリング.複数画像間の対応点検出(SIFT特徴量など)と,幾何計算による3次元形状の復元. 応用:写真共有サイト(Flickrなど)の写真から都市の3次元モデルを作る AutoDesk(デジカメの画像から3次元モデルを作成するWebサービス) 次の課題:風景画像からその画像に移っている建物の位置を推定する,など 市街地時空間モデリング(東日本大震災の被災地の時間的変化を丸ごと記録)・物体認識:機械学習によりさまざまな対象の認識が可能に DNNによる特徴学習:特徴量と識別機を同時に学習する  畳み込みニューラルネット(ネオコグニトロンの末裔) 画像特徴の教師なし学習おばあさん細胞の生成を確認 腹側皮質視覚路の処理と近い サルの脳と視覚の研究:サルはカテゴリ的な認識が可能・その他難しい逆問題 同一物体のセグメンテーション,人間のポーズ推定,表面反射,など(不良設定問題) Blind debrurring (ぼけ除去)1-4-10 (招待講演)映像意味検索技術の最新動向 篠田 浩一(東工大)・コンシューマのビデオデータの問題点(メタデータがない,品質が低い,など) テレビや映画のコンテンツ分析と比較して難しい・音声と映像の共通点:通信モデル,「語彙と文法」 生成モデル,確率的方法論,高速な計算・TRECVIDワークショップ データが提供される,技術の比較が可能 2012年のタスク:Know Item Search, Semantic Indexing, Surveillance Event Detection, Instance Search, Mutimedia Event Detection, etc.・Semantic Indexing (SIN) ショットの中から概念を抽出する(映像,音声)「概念」は346(物体,イベント,シーンなど)  概念の出現頻度は全く違う 600時間で学習,200時間で評価 従来法:Bag of Words (BpW) 特徴量はSIFTとか 新しいトレンド  たくさんの特徴を使う,マルチモーダル,キーフレーム以外のフレームも使う,ソフトクラスタリング  高速化(近似,並列化,GPU)  思ったより使えなかった特徴:画像全体の特徴,音声認識(概念と関係なかった),文字認識,物体の位置(推定が難しい),概念間のコンテキスト  篠田先生の研究室での研究:GMM,MAP適応,木構造サーチ(話者認識と類似した技術)  関連研究:Sparse coding, VLAD(vector of locally aggregated descriptors), Super-vector coding, Fisher kernel・Multimedia Event Detection (MED) 複雑なイベントの検出(野球で点を取る,ケーキを作る,等) SINの手法をそのまま使っている 時空間特徴量は強力,音声認識は有効でない SINで検出した概念を並べて,それを特徴として認識を行う1-4-11 (招待講演)コンテンツを見ないコンテンツ内容理解へ向けて−人間行動から読み解くコンテンツ 木村 昭悟(NTT CS研)コンテンツを見ない理解:画像と説明があれば,画像そのものがなくても説明からどんな画像かわかる周辺情報で変容するコンテンツ:画像にキャプションがあると画像の持つ意味が変わることがあるMalcolm SlaneyのIEEE Multimediaでのレビューから 音楽の類似度:内容に基づく類似度と,ユーザのレーティングに基づく類似度では,レーティングの方が高性能 映画の推薦:協調フィルタリングのスパースネスを補うのに,内容よりも日付の方が重要だった 不適切画像のフィルタリング:テキスト,画像,Web上の関係を使うと,Web上の関係に基づくスコアがよい教訓 対象コンテンツだけを解析することの危うさ(ある種の文脈ではコンテンツよりメタ情報の方が重要) コンテンツが当てにならない場合がある(感性,感情,嗜好などを扱う場合) 何を頼りにすべきか  人間の行動,人間が行動する場の構造に着目コンテンツを見ない内容理解のススメ システムを利用する人間の行動観察 人間の行動解析による付加情報獲得 獲得した付加情報をコンテンツ解析に使う:コンテンツを深く解析するための弱い教師情報事例・画像検索のユーザ行動を画像アノテーションに生かす 画像検索サーバへのアクセスログのみから画像にラベルを付与  単に「画像をクリックした」というだけでなく,クリック直前の滞在時間など前後の行動を使う・SNS画像の有用性を推定 Pinterestでの"pin"行動から画像集合の類似関係を求める画像特徴量のためのコーパスとして利用最後に (Malcolm Slaleyのレビューから) コンテンツそのものを無視すべきでないが,周辺情報を無視することも得策ではない 人間が生み出すシグナルに耳を傾けることが重要
特別講演2 (09:00-10:00)(6) 私の音声研究とSLP   鹿野 清宏最初に鹿野先生の略歴。NTT(CMU)ATRNTT奈良先端各時代での研究と関連する人たち。ATR時代。Alex Weibel、東倉先生、Victor Zue、Frank Fallside
SLPの遠い前身のATR勉強会(@温泉)。ATRで開発した各種技術(TDNN,Speaker Adaptation, etc.)NAIST時代。輩出した博士が33名、修士が約200名、論文が160件、国際会議が450件。JNAS、IPA日本語ディクテーション基本ソフトウェア、音声認識基本技術講習会連続音声認識コンソーシアムBSS、ASKA&たけまるくん、キタちゃん、平城遷都1300年最後にGoogle Scholarでの各分野のランキング。テーマセッション「音声言語処理技術の今後」:音声合成(10:15-11:45)(7) 音声合成は今後こうなる!   徳田 恵一,峯松 信明,戸田 智基,額賀 信尾,平井 啓之・徳田先生最近の技術動向  単位選択型音声合成:高品質、多量のデータが必要  統計的パラメトリック(HMM)音声合成:多様性、方式が言語非依存、低メモリ量  ハイブリッド型多様性の実現  話者適応、話者補間、固有声、感情音声合成、歌声合成(マツケンサンバ)実世界での利用  カーナビ、電話音声案内、音声ポータル、ボイスサーチ、音声翻訳、etc.今後の技術開発  統合モデル    波形レベルの統計モデル    テキスト解析部との統合これまでの音声認識研究は音声合成のためだった?ユニバーサル音響モデル:あらゆる話者性・発話スタイル・感情表現・言語等を自在にモデル化・制御可能な音声モデル  学習データをどうするかが問題ユニバーサル音声モデルと超大規模音声データ  音声モデル構築のための技術的基盤+多様な音声データを蓄積・共有する社会的基盤が必要社会的基盤  分断された音声データの共有化:医療(ボイスバンク)、エンターテイメント、商業分野、学術分野(Blizzard Challange)などで共有できないか  Web上の音声データの利用(Podcast, Audiobook)音声コンテンツ制作支援  読みやアクセントの修正、抑揚の編集、歌唱表現の編集など今後の音声合成  高品質化・多様化が着実に進み、音声インタラクションシステムとしての洗練・適応も進むことにより、日々の暮らしに溶け込んでいく・峯松先生ここ数年、音声合成ソフトの教育利用が増えている  電子化辞書、教科書読み上げ、会話エージェント語学教師が感じている合成品質:読み上げ能力としては超上級者相当実現しようとしている「母語話者らしさ」:人間は「通じればよい」音声合成は「母語話者のようでなければならない」「どう読むか」を支援する  テキストに明示されていない情報(韻律など)を視覚的に表示OJADスズキクン of OJAD:日本語韻律情報の教材対話音声合成ってCALLに必要?  Communicativeな外国語教育:文法重視からコミュニケーション重視へ    焦点・意図などが重要  社会生活を営むための音声教育が求める技術(日本語の場合)    外国語訛りが「態度」として受け止められる、若者言葉の使用の問題TPOにあった「話し方」を教えるツールとしての音声合成技術英語を対象にすると  さまざまな「英語」:発音を「正しく」する必要があるか?他社の英語を「聞き取りやすい訛り」に変換するインフラそれぞれの人がそれぞれの英語でコミュニケーション個人単位で外国語能力を管理する  CEFR: 言語パスポート(個人の言語能力証明)・戸田先生音声変換技術とは 音声音声、または(音声+テキスト)音声基本的な枠組み パラレルコーパスによる統計モデル:   コードブックソフトクラスタリング線形回帰GMM結合pdf学習ML系列変換+GV各種確率的枠組みへ   フォルマントシフト、セグメント単位選択フレーム単位選択 適応技術の発展 応用技術の発展   個人性言語間感情帯域拡張雑音抑圧調音逆推定電気喉頭音声変換・・・音声変換技術を今後こうしたい  非言語情報を制御物理的・身体的制約を超えたコミュニケーションの実現    いかに意図をシステムに伝えるか  発声障碍者補助、サイレント音声合成、コンテンツ創作  品質を許容レベルに抑える(極端な劣化がない)  物理的な制約の導入  リアルデータの収録の枠組み作成  データとモデルのミスマッチを補正する  パラ言語情報の生成・制御  主観的・知覚的特性との対応のモデル化(創作支援)  音声データベースの拡充  音声変換技術の社会的認知を広げよう・額賀さん日立の音声合成技術と今後の展開。日立の音声合成技術の歩み:アナログ合成LSI(符号化方式)単位選択型セレクティブ重畳方式(波形の韻律が連続的になるための接続方式)Ruby Talk(カーナビ用記号規格JEITA TT-6004)、ボイスソムリエ(ナレーション作成、韻律手動制御)音質向上をドライブした要因  計算機リソースの充実  ユーザが厳しい  良いコンペティタがいた  ツール類の充実今後の課題・方向性  人間の音声との差の解消(特に対話システムに利用した時の不自然さ) 会話調音声のアノテーション・制御  Controllabilityの獲得(想像した声を作り出す)  話者が希少な言語、書き言葉がない言語の保存と合成・平井さんエーアイの音声合成の現状と今後について。H15にCHATRの販売会社からスタート。自家製エンジンAITalk作成。J-ALERTに採用、「大沢たかおのあなたに朗読」、しゃべってコンシェル日本語に特化したエンジン:少ない収録音声から安定した高品質な音声の合成が可能専門の技術者による精密なラベリング(半自動)カスタムボイス:有名人などの音声合成器を提供(2時間～6時間の音声)予算に応じて作成  サンプルボイス(吉田君、ふなっしー、ガチャピン/ムック)  関西弁対応現状の課題  合成音声の品質が話者に依存して劣化(少ない収録量で顕著)  劣化要因:読み方の安定度(滑舌、感情)、ピッチマークのずれ、PSOLAの波形変形による劣化今後の目標  利用分野を広げる:コンテンツ作成・ディスカッションHMM音声合成と波形接続の関係の今後は?ハイブリッド型(HMMで単位選択、HMM合成結果と単位選択結果の混合)など。 ハイクオリティなら単位選択、多様性を求めるならHMM(徳田) 韻律等には統計的モデルが利用できる。ボコーダの品質は問題。そこがクリアできれば(平井、額賀)汎化誤差が0にならない問題に対して音声合成ではどう対応するか?うまい軸を設定すれば汎化の問題は何とかなると思う(徳田)音声合成に使われている各種基盤技術は日本初(嵯峨山)スペクトルベースの方法はどの程度になったら使えるのか(森勢)「そちらを使ったほうがメリットがある」ということが必要。読み上げならば波形接続で十分(平井)日本語のテキスト解析の現状は(西沢)テキスト解析の部分も言語非依存にしたい。基本的な構造だけを使って機械学習という方法で収束させたい(徳田) Blizzard Challangeでインドの言語を一か月で作るチャレンジをしている(徳田) 言語教育の場合は教えるのは人間なので、言語依存なところにも意味がある(峯松)インダストリーセッション(13:00-14:00)庄境誠 (旭化成), 西村雅史 (日本IBM), 大淵康成 (日立/クラリオン), 河村聡典 (東芝),越仲孝文 (NEC)・西村さんIBMの研究成果。ViaVoice(1996)累計200万本音声理解システム、裁判所システム(コスト面で失敗) 現在も毎日血の涙を流しながら認識率向上に努力している車載用音声理解システム(2008)、コールモニタリングシステム(2009)、クラウド型音声検索サービス(2011)など音声認識はクラウドへ(組み込み型認識はニッチへ)次はCognitive Computingの時代・大淵さん日立グループの音声言語処理ビジネス。カーナビ型ミドルウェア携帯型音声翻訳装置耐雑音音声認識車載向け音声対話マイクアレイ音声認識実用化PJビッグデータ応用STD音声強調・VAD車載向けクラウド音声認識(エンジンはGoogle)音声認識ビジネスのこれから:音声I/Fと音声ビッグデータUI設計・対話機能/認識エンジン(巨人に対抗)/パラ言語解析他・河村さん東芝の音声言語処理ビジネス。過去:LaLaVoice、音声ミドルウェア(カーナビ用)現在:音声合成オンラインサービス ToSpeak Online(自分の声で合成器が作れる)                  音訳システム Daisy Rings   音声認識クラウド利用B2Bソリューション 音声つぶやきシステム(作業中に音声でログを取る)                        書き起こし支援システム ToScribe未来:音声合成単なる読み上げから、会話・コンテンツ作成・エンタメ系へ        ビジネスのためには編集コスト削減も重要   音声認識CPS(Cyber Physical System) 人間のコミュニケーション活動のセンサーとしての音声認識   音声インタフェースコグニティブアシスタント Xpressive Talk(感情音声合成+トーキングヘッド)のデモ・越仲さん過去:音声タイプライタ、音声翻訳などを開発してきた現在:音声入力端末 VoiceDo(品質管理など) 議事録作成 VoiceGraphy キャラクタ会話「おしゃべりシナモン」   裁判音声認識システム(2009) 検察取り調べの録音の認識、話者認識未来:ICTを利用した社会インフラの高度化領域へ経営資源を集中   「音声言語」から「音声・音響理解」へ     Speech-to-Text技術は継続的に注力     Speech-to-X, Sound-to-X・庄境さん過去:部品ソフトウェアビジネス VORERO 累積424万ライセンス N自動車のカーナビを作っているC社が採用   多言語に対応しきれないので撤退現在:統合ソフトウェアビジネス 業務用音声ソリューションパッケージVOHMIAK   音声対話部分VOHMIAK PD-2 +モトローラ業務用タブレット 音声対話による業務支援   システム導入の経済効果の定量化が可能、ユーザへの訴求未来:社会インフラ、社会システムを支える音声でないとビジネスにならない   業務の中で話されている音声を処理して価値を与えるソリューションを探すJEITAの調査: 医療応用(パーキンソン病を音声で診断支援)調剤薬局の薬剤師支援 等・ディスカッションつぶやき声認識はどの程度のレベルにあるのか?(小林)業務を限れば文字正解率9割程度。日常での会話はまだ無理(河村) 通常の会話音声の認識には証跡としての価値がある。ターゲットを絞ればLVCSRである必要はない。(庄境) STDではかなり自然な会話でも精度は9割。再現率はまだまだ。精度が高ければよい応用から攻めたい(大淵)認識エンジンがクラウドになっていくときに、音声技術企業としてどう対応するのか。(中村)これからはエンジンの時代ではない。その周辺のVADやエンハンスメントが競争力の源泉になる。音声対話も。(大淵) セキュリティが重要な分野では独自エンジンもある(大淵) IBMでは認識エンジンに力を入れている。データもできるだけたくさん利用する。あらゆる組み合わせを試している。(西村) NECでも認識性能に力を入れている。データでどう勝負するかは考えていかなければならない。他人がとってこられないデータをどう確保するかがポイント(越仲) Googleのエンジンは強力だが、Googleが及ばない世界がある。業務の世界はGoogleからは遠い。(庄境)日本で今後はそれぞれのメーカーで音声技術が生き残るのか。それとも1社に縮退するのか(中川) 音声認識は市場の期待に全く応えられていない。話し言葉はまだまだできない。そちらを進めればまだまだ行けると期待(河村) 音声技術を単体で売ると大した額にならないが、大きなシステムの本質的な一部になれば大きなビジネスになりうる(大淵)若者をエンカレッジする意味で何か一言ずつ。(徳田) 音声認識の研究は終わっていない。それを追求すればまだすべきことが多い。(西村) 「音声認識」で何をイメージするか。その技術はspoken languageだけのものではないのではないか。ほかの分野にも活用ができる部分があるので、そちらでも若い人に頑張ってほしい。(庄境)いま中国の音声市場は盛り上がっている。日本の状況はどうか。(山岸) 世界では音声研究者の取り合いになっている。日本でも、お客さんを見る限りニーズは高い。日本・世界分け隔てなく高いポテンシャルがある。しかし音声認識技術はそのニーズを救い上げるだけのポテンシャルがない。(河村) いま音声を含む人工知能ビジネスは世界的には旬。それと比べると日本は弱いが、音声技術への期待はViaVoiceの時代よりはるかに高い。(西村)テーマセッション「音声言語処理技術の今後」:音声応用(14:15-15:45)(8) 音声応用(音声対話)は今後こうなる!   中野 幹生,李 晃伸,駒谷 和範,東中 竜一郎(9) 音声言語処理を利用した情報検索評価タスク: NTCIR SpokenDocからSpokenQuery&Docへ   秋葉 友良音声対話と音声検索の現在から未来へ。・秋葉先生音声ドキュメント処理ワーキンググループ (2006～2012)目的:音声ドキュメントを利活用する技術の開発を目指し、研究基盤を整備する。活動内容:データベース整備、テストコレクションの開発・公開、ワークショップの開催WGによる音声ドキュメント検索テストコレクションの構築(2009～2010)NTCIRにタスク提案(2010)  NTCIR: 検索システムの力比べNTCIR-9 SpokenDocとNTCIR-10 SpokenDoc2  音声検索語検出と音声内容検索タスクを同時に実施  可変長区間パッセージ検索  ISTD(単語が検出しないことの確認)タスク従来のテキスト情報検索では「検索窓へのキーワード入力」パラダイムから脱していないのではないか。音声検索はそれを打ち破る可能性がある。長い自由発話音声クエリによる音声ドキュメント検索 NTCIR-11 SpokenQuery&Doc・中野さん音声対話システムの流れ ATISCommunication/Error Handling/話題管理/意図理解/話者交代/POMDP今後(予想)音声対話システムはこうなる どんな場面で対話システムが使われるか?重要な形態は限られる   音声が重要な局面では多人数マルチモーダル対話システムが有力だろう研究テーマはこうなる 深い意味理解・実世界にグラウンドした意味の理解・状況依存対話(なるべく音声を発しなくても理解されるシステム)研究体制はこうなる 対話研究者はそれぞれの要素技術(音声・言語・画像・知識・インタラクション・ロボット)の統合に集中せざるを得ない音声入出力ツールはこうなる 音声のことをよく知らない対話研究者が簡単に使えるツール テキスト入出力ではない特徴の利用   リッチな音声音響センシング(音声と認識結果、音響イベントなど)、多様な音声出力(感情、態度、パーソナリティなど)その他こうなるかも 「音声対話システム」という言葉は使われなくなる(マルチモーダルが当たり前) 「対話システム」AIエージェント? 対話研究者は音声認識率を気にしなくなる?・李先生音声IF・音声対話の応用に向けて音声応用の現状:現在は注目の山を登っている         次に来るものは?キャラクター対話、モバイル・ユビキタス、オームオートメーション、エンタメ音声モーダルの楽観的予測・悲観的予測   悲観的予測は音声認識率がどんなに上がっても変わらない?(話しにくさ)   話しかけにくさは音声応用における重要課題の一つ話しかけにくさの原因:切り分けが難しく、個別のノウハウになりがち ユーザに自由な発話を許すと、音声言語の外にある諸要素の影響が大きくなる   実際にモノを作ると「センス」が問題になるツールキットによる実証的アプローチ   音声言語処理技術とデザインの分離MMDAgent 耳、口、骨、皮、空間を実装した研究用プラットフォーム 他分野の人が使える音声認識・合成基盤ソフトウェアコンテンツの作り手からの普及  能動的ユーザを創り出す:音声対話の理解者の増大いまなにをすべきか  作って、使ってもらうことを通して研究につなげる  技術とデザインの接点を模索する20年後  関連分野との新たな連携(音声対話デザイン論、音声対話心理学)  コンテンツ業化、「音声対話デザイナー」が成立・駒谷先生言ったことの理解から意図の理解へSituated Dialog(ある状況・コンテキスト下で動く対話)  従来は入力音声を分析理解する部分のみを対象にするが、発話には信号・言語・社会などのレイヤがある。社会的制約の利用  人間同士の対話タイミングを人間ロボット対話に利用、など意図の理解へ  ユーザ・周辺環境を含めた系を理解する  すべての情報を言葉で支持するのは煩わしい言ったことを理解するだけでなく「状況」を理解する必要がある必要なこと  1.相手のモデルを持ったシステム:ユーザへの適応、ユーザが適応  2.膨大な背景知識の構築とその獲得:ドメイン知識のプロトタイピング、知識獲得  3.発話状況の理解:マルチモーダル化、状況理解(信号レベル、社会レベル)「言ったこと」の理解から意図の理解へ・東中さん雑談対話システムに向けて現状認識  タスク指向型対話システム:2000年代初頭から変わっていない、モジュールの進化とロバスト化  雑談(オープンドメイン対話):体系的研究はなく、アドホックに実装20年後はこうなる  音声はタスク達成のためのモダリティとして確立、日常に入り込む(技術を意識しないレベル)  オープンドメインの発話内容を理解する技術が確立  発話内容の記録に基づいたシステムなぜ雑談対話システムなのか  対話はもっと多くの用途に利用できる  ユーザは実際に雑談をする傾向がある  いずれオープンドメインな理解系が必要になる雑談の有効性  システムとの社会関係、愛着、ユーザ情報の獲得、指向喚起・承認必要となる技術  照応解析、述語項構造解析、談話関係認識、トピック認識、結束性判定、含意認識  コーパスがほとんどない。コーパス作成から要素技術の確立に10年、要素技術の統合に10年  20年度は発話内容の記録(聞き役)はできそうNTTの雑談対話システム  ドコモ ドライブネットインフォに実装  おっさんと雑談雑談対話の評価型ワークショップが必要対話エージェントに人格や個性が必要なのではないか?(嵯峨山)しゃべるときには相手に人格を投影することが必要。それをどうモデル化・表出するかが問題(李) 人格が破たんする応答をすると満足度が下がる。しかし人工的に人格が与えられるかどうか疑問。共同作業者として予測可能な人格は必要だが、それ以上のものがあるべきかどうかについては議論が必要(東中)人間を超える能力を持った対話システムについてはどうか?(後藤)対話は人と人のプロトコルなので、それを超えるプロトコルは人が受け止められないのではないか(駒谷)意図理解などの用語技術はこの20年ほとんど進んでいないと思うが、どうか(中川)言語理解は進んでいないが、Webによってだいぶ状況が変わってきている。今後はウェアラブルと言語に注目している(東中)日本語特有の省略などから独自の「場を読む技術」というものが出てくるかどうか(峯松)日本語特有の照応問題などがあるので、そこから独自の技術が現れる可能性はある(中野)Situated dialogなどは、以前のICOTなどでやっていた問題と基本的には同じ。システムは「場を読まない」ことが許される。人間にとって透明であればいいのでは。(河原)システムよりも人間のほうが場を読めない。機械が人間を止めにかかるようなこともあるのでは。言語処理は80年代以来ずっと表層をやっていて成功したが、また深い理解が必要になる。コーパスを集めなければならないが、何かアイデアはないか(小林)アメリカではディスコース系のコーパスはあって、小さいがブートストラップには使える。実際にはシステムを作ってリリースするほうが早い。(東中)
特別講演1 (13:00 - 14:00)(1) 音声言語をキーワードとした40余年の研究生活   中川 聖一・SLP創設前後の動向1985にCMUに長期出張で刺激を受ける。1987頃に手弁当で今後の研究についての議論。1988科研費総合研究Aの申請・採択。1989活動開始。1986ATR創設。1987重点領域研究「音声言語」。1988合宿形式の討論会。信学会2種研究会時代の話。音声だけでなく画像・文字認識などさまざまな分野の人との交流。1992情処学会「音声言語情報処理と音声入出力装置研究グループ」創設。1994音声言語情報処理研究会発足。SP・NLCとの関係。主な活動:大語彙音声認識WG、情処特集号、日本語ディクテーション基本技術講習会、Spoken Language Processing 出版・論文賞になった4つの研究紹介1977 D論の論文化。記号系列間のDPによる単語スポッティング。CFGによる言語モデル。ビームサーチ、話者適応。1988 Stochastic Dynamic Time Warpingによる音声認識。DPとHMMの統合。2000 サーベイ論文「音声認識研究の動向」人間と機械の認識能力の比較。trigramは人間に近く、音響モデルは人間に及ばない。・最近の話題であるニューラルネットワークの再考1989 「HMMとニューラルネットワークの接点」現代の視点からの補足。最後に「人間の脳とニューラルネットは関係あると思うか」について去年やったアンケート結果の披露。テーマセッション「音声言語処理技術の今後」:音声認識 (14:15-15:45)(2) 「音声認識」は今後こうなる!   篠田 浩一,堀 貴明,堀 智織,篠崎 隆宏(3) 音声認識の方法論に関する考察—世代交代に向けて—   河原 達也河原先生が「20年ぐらいキャリアがあって、さらに20年ぐらい研究しそうな人」を人選。・河原先生:去年の「音学シンポジウム」の講演と似た話「標準的な」音声認識の定式化。Noisy channel model+最尤推定+大規模コーパス。しかしそれらが現在揺らいでいる。大規模コーパス:数十時間(90年代)数百時間(2000年代)数千時間(2010年代) 規模の伸びは指数的だがかかるお金は規模に比例   「データをがんばって集める」枠組みの限界リアルなデータを自然にかつ超大規模に集積できる枠組み    キラーアプリ(音声サーチなど)を無料で公開、会議・講演を蓄積モデルの最尤推定:複雑怪奇なモデル  識別モデルの導入(DNNなど)Noisy Channel Model:対数線形モデルへ(統計的機械翻訳が通った道)さまざまな情報を統合音声認識の世代交代(第4世代)・篠田先生篠田先生の心象風景。20年前はblue oceanにHMMとN-gramが浮かんでいたが、現在はGraphical Model/WFSTの陸の一部。沖にはDNNの島。20年後はおそらくその部分も陸になっているだろう。ハードウェアがソフトウェアを規定する。量が問題。新しいアプリケーション、ユーザが学習する20年後:コンピュータは速くなりつづける。センサ技術の発達によりモダリティの違いに意味がなくなる?事例ベース、Product of Experts, 隠れ変数はなぜ必要なのか?センサークラウド・堀貴明さん今後のトレンド:ユビキタス、ウェアラブル、パーソナライズ音声認識技術の進歩と今後:日常会話音声、遠隔発話日常会話野認識字幕、議事録、ライフログ個人・集団のアイデアや行動の履歴としての価値現在の技術は一入力一出力(単一話者仮定)将来は多入力他出力で相互作用もモデル化する。多入力他出力系=ニューラルネット?30年後:一を聞いて十を知る音声認識 予測変換のような技術(長期履歴に基づく補完)・堀智織さん音声言語処理研究分野の発展現在のNICTでの音声インターフェース:多言語翻訳、高齢者支援多言語音声翻訳 U-STARアジア太平洋地域での音声言語翻訳の展開 2015年までに生活会話の翻訳音声自動インデキシング・字幕付与研究 多言語字幕付与システム 多言語音声翻訳を用いた聞き耳システム音声認識は水道のようなもの:いつでもどこでも音声インタフェース多言語音声対話システムビルダー:音声技術開発者とアプリ開発者の橋渡し東京オリンピック2020に向けた音声認識・翻訳技術・篠崎先生音声認識研究の難しさ: 音声認識率のインパクト低下、ベースラインシステム作成の困難化新しいステージへ 現在の音声認識技術は「カンブリア爆発」の直前現在の音声認識に足りないもの 明らかに機械なのに妙に生物っぽい怪しさ(こっそり言ったことがいつのまにか認識されている等)  学習・適応化プロセスの自律化 ワンタッチを上回る直感性および正確性  自己完結な音声インタフェース(耐雑音、ハンズフリー) 組み込み部品としての使いやすさ  低消費電力音声認識技術、ワンチップ化、ワイヤレスセンサ化新しい方法論 BCIを使った音声認識過程のリバースエンジニアリング(動物実験) 分野交流 バイオインフォマティクス・河原先生によるまとめ何ができる(what): 中期的には講演音声、放送・ネット音声、ロボット対話 長期的にはプライベートな会話、ドラえもん、「一を聞いて十を知る」 方言への対応どうできる(how) 生成モデル vs. 識別モデル 30年後もDNNやN-gramが使われているか?「気持ちの分かる」音声認識?(嵯峨山)ショート発表 (16:00-18:00)(4) SIG-SLP第100回記念シンポジウム:ショート発表最初に齋藤さんがオープニングトーク。第一部 (16:04-16:40)モデレータ: 森勢 将雅 (山梨大)鈴木 直人 (東北大): "ARキャラクタとの英会話練習時における交替潜時のタイムプレッシャーによる制御" キャラクタとの会話による英会話練習。ARを使って現実を参照しながら英会話練習をする千葉 祐弥 (東北大): "マルチモーダル情報を使った音声対話システムのユーザ状態推定" もう少し社会性のある対話システムがほしい。そのために相手の心的状態(考えている、戸惑っている)を音声と画像から推定。中島 陽祐 (名工大): "クラウドソーシングを用いたインタラクティブな音声対話システムのための大規模主観評価プラットフォームの構築" MMDAgentのようなシステムのインタラクションに対する大規模な主観評価をしたい。そのためクラウドソーシングによる大規模実験のプラットフォームを作った。西村 良太 (名工大): "楽しい音声対話システムを作りたい!" 中川研ドクター時代は雑談音声対話システムを作成。特にリズムを重視。名古屋大ではデータベースの音声検索システムを作成。現在は名工大でMMDAgentベースの音声対話システムの構築を目指す(ブラウザ上で作れる対話シナリオエディタ)。原 直 (岡山大): "地理情報を活用したモバイル音声対話システムに関する研究" 自分用の音声対話システムを持つのが当たり前の世界へ。ブログやSNSのように誰でも作れて公開できる音声対話システム。設置型の(たけまる型)システムをモバイルに移植。現在位置に依存して推薦情報を変える。マルチエージェント強調による推薦、ライフログからの情報自動生成。堀田 尚希 (名古屋大): "発話の誤分割を修復する音声対話システム" 発話の途中で言いよどむことによって途中でシステムが反応してしまう(発話の誤分割)への対処。松山 洋一 (早稲田大): "多人数会話ファシリテーションロボット" 人間3人の会話の中にロボットが4番目の話者として参加し、会話に干渉して全体の場をコントロールする。吉野 幸一郎 (京都大): "ユーザの焦点に適応的な音声によるニュース案内システム" ユーザの曖昧な要求に対して対処するために、焦点(ユーザの興味など)を導入したPOMDPによる対話のモデル化を行っている。「空気の読める対話システム」ユーザが言いたいことがうまく言えないときにシステムが主導してくれる。Yuan Liang (東工大): "Error Correction Interface for Speech Recognition" 音声認識結果の修正まで考えて、単に認識率が高いだけでなく、修正したときの手間が少なくなる認識結果を優先する。第二部 (16:44-17:20)モデレータ: 原 直 (岡山大)川渕 将太 (名古屋大): "音楽における個人性の信号処理的モデル化" バネ質量系を用いた合唱歌唱のモデル化(F0の引き込み)。類似楽曲検索(楽曲間の主観的類似度の推定)。そのための主観評価データの分析(数量化理論III類)。小林 和弘 (奈良先端大): "統計的手法に基づく歌声声質変換" 声質を自由に変えられるボーカルエフェクタ。対象歌手がいる場合と、具体的な対象がなくて表現語を使う場合があり得る。前者では固有声GMMによる多対多声質変換。後者は修正重回帰GMMによる声質変換(制御パラメータは年齢)。白鳥 大樹 (山梨大): "危機的状況を瞬時伝達する警報音の合成を目指した音響パラメータ制御の検討" 叫び声に固有のパラメータを強調した音声を警告音に使うことでより警告らしさを強調したい。そのために叫び声のパラメータを分析中。音声をTANDEM-STRAIGHTで分析し、フォルマントとF0を操作する。田中 宏 (奈良先端大): "スペクトル補正及び統計的音源生成に基づくハイブリッド電気音声強調法" 電気式人工喉頭の音声の自然性向上手法。特に劣化要因をなくすことが重要。統計的声質変換によるF0変換と、スペクトル減算による音声強調の組み合わせ。田中 宏季 (奈良先端大): "自閉症スペクトラム児と定型発達児のナレーティブ発話分析" 自閉症者支援のために、コンピュータによる「共感(エンパサイジング)」の認知と表出の補助を行う。俵 直弘 (早稲田大): "MCMC法に基づく話者モデリング" 「混合混合ガウス分布」による話者クラスタリング。GMMの混合になるが、その混合数も推定するためにディリクレ混合分布モデルを利用。MCMC法による推定を行う。西田 昌史 (同志社大): "最近の研究内容について" NMFによる話者クラスタリング:階層型クラスタリングに基づく方法よりも高精度。音声入力による音声ドキュメント検索:ベイズリスク最小化音声認識による評価中。マルチモーダル会話分析:多人数でCALLシステムと会話するシステムを構想。言語によって多人数会話時の特徴がどう違うかを分析中。橋本 浩弥 (東京大): "日本語アクセントに基づく基本周波数パターンの区分線形回帰とHMM音声合成への適応" 長時間に現れる韻律的特徴をモデル化。句に対するF0変動を音節とそのアクセント(H/L)を使って区分的線形関数として近似する。音声合成のF0生成に応用。Sangeeta Biswas (東工大): "Clustering i-Vectors for Training PLDA Models in Speaker Verification" i-Vectorを使った話者照合。PLDAモデルを学習するときに、話者クラスタに依存した学習をする。第三部 (17:24-18:00)モデレータ: 齋藤 大輔 (東京大)秋田 祐哉 (京都大): "音声言語処理技術を用いた講義・講演の字幕付与" 講義・講演に字幕をつける。オフラインで正確な字幕(アーカイブ)と、リアルタイム性が重要な字幕(ライブ)がある。アーカイブ用には、できるだけ正確な音響モデル・言語モデルを使って編集もできるシステムを開発した。リアルタイム字幕については、リアルタイム音声認識+有用性の判定により人手による編集を効率化。市川 賢 (名古屋大): "クエリ拡張と音節認識結果を併用した音声ドキュメント検索" 音声ドキュメント検索の未知語対応のために、クエリ拡張と音節認識結果を併用する。桂田 浩一 (豊橋技科大): "高速で高精度な音声ドキュメント検索システムの開発と試験運用" 放送大学を対象とした音声ドキュメント検索のプロジェクト紹介。音声認識(中川、新田)+音声検索(西崎)+高速なSTD(桂田)+音声コンテンツ検索システム(秋葉)。柏木 陽佑 (東京大): "Deep Neural Networkを用いたクリーン音声状態識別による雑音環境下音声認識" DNNを使った雑音除去。NNによってクリーンかどうか識別。学生交流のすすめ。張 聡穎 (東京大): "Using Phonetic Context for Continuous Speech Recognition with Invariant Structure" 構造的特徴による連続音声認識。従来の認識結果を構造的特徴によるスコアでリランキング。従来の構造的特徴はmonophoneだが、この研究ではtriphoneを使う。ポンキッティパン ティーラポン (東京大): "日本人英語音声を対象とした単語了解度の自動予測" ERJの単語了解度を自動予測。ERJ音声の聴取実験結果から各単語の了解度(人間による認識率)を測り、音響的特徴から了解度を予測する。長野 雄 (東北大学): "省リソースな計算機のための音声認識における演算量の削減" Raspberry Pi上での音声認識のための計算量削減。増村 亮 (NTT): "言語モデリングにおける学習データの課題を解決するための2つのアプローチ" N-gramの基本性能・利便性をあげるための研究。Web上の言語資源からCSJ並の言語モデルを作る。またバックオフの限界を超えるため、複雑な構造の言語モデルから生成した文を使って通常のN-gramを学習する。森勢 将雅 (山梨大): "Deep neural networkによる音声認識に適した特徴量抽出の検討" DNNで音声の「好感度」に相当する特徴量を取り出したい。現在試し中(20:00-21:30)(5) 音声言語情報処理研究会の20年—歴代主査による研究レビュー—   新田 恒雄,小林 哲則,中村 哲,武田 一哉,河原 達也,伊藤 彰則司会だったのでメモを取り忘れた。それぞれの主査が昔を偲んで講演。研究レビューのつもりだったけど自分の研究紹介的な話が多かった。時間をちょっと超過して終了。
